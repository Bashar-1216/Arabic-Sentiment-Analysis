<![CDATA[<div align="center">

# ğŸ‡¸ğŸ‡¦ Arabic Sentiment Analysis Using Deep Learning ğŸ§ 

![Python](https://img.shields.io/badge/Python-3.x-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![HuggingFace](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Transformers-yellow?style=for-the-badge)
![License](https://img.shields.io/badge/License-Academic-green?style=for-the-badge)

> ğŸ”¬ Ù…Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚

**A comprehensive Arabic Sentiment Analysis project comparing BiLSTM with CAMeL-BERT embeddings against a Fine-tuned CAMeL-BERT model.**

---

</div>

## ğŸ“‹ Table of Contents

- [ğŸŒŸ Overview](#-overview)
- [ğŸ“‚ Project Structure](#-project-structure)
- [ğŸ“Š Datasets](#-datasets)
- [ğŸ§¹ Text Preprocessing](#-text-preprocessing)
- [ğŸ—ï¸ Model Architectures](#ï¸-model-architectures)
- [ğŸ“ˆ Results](#-results)
- [âš™ï¸ Requirements](#ï¸-requirements)
- [ğŸš€ How to Run](#-how-to-run)
- [âœ¨ Key Features](#-key-features)
- [ğŸ› ï¸ Technologies Used](#ï¸-technologies-used)

---

## ğŸŒŸ Overview

This project implements and compares **two powerful deep learning approaches** for Arabic Sentiment Analysis:

| # | Approach | Description |
|:-:|----------|-------------|
| 1ï¸âƒ£ | **BiLSTM + CAMeL-BERT** | A Bidirectional LSTM model leveraging pre-trained word embeddings from CAMeL-BERT |
| 2ï¸âƒ£ | **Fine-tuned CAMeL-BERT (LLM)** | Direct fine-tuning of the CAMeL-BERT model for sequence classification |

> ğŸ¯ Both models are trained on the **AraSenti** dataset and evaluated on an out-of-domain **HIA Qatar Airport Tweets** dataset to assess **cross-domain generalization** capability.

---

## ğŸ“‚ Project Structure

```

â”œâ”€â”€ ğŸ““ Arabic-Sentiment-Analysis.ipynb          # Main Jupyter Notebook (training, evaluation, comparison)
â”œâ”€â”€ ğŸ“Š AraSenti_all.xlsx          # Training dataset (~15,751 samples)
â”œâ”€â”€ ğŸ“Š HIAQatar_tweets.xlsx       # Testing dataset (~151 samples)
â””â”€â”€ ğŸ“ README.md                  # This file
```

---

## ğŸ“Š Datasets

| Dataset | Description | Samples | Labels |
|:-------:|:-----------:|:-------:|:------:|
| ğŸ“— **AraSenti** | Multi-source Arabic sentiment corpus | 15,751 | Negative (0), Positive (1), Neutral (2) |
| ğŸ“˜ **HIA Qatar Tweets** | Airport-related Arabic tweets | 151 | Negative (0), Positive (1), Neutral (2) |

> âš ï¸ **Note:** The model is trained on AraSenti and tested on HIA Qatar Tweets to measure **cross-domain** performance.

---

## ğŸ§¹ Text Preprocessing

Arabic text is cleaned using the `clean_tweet()` function which performs the following pipeline:

| Step | Operation | Description |
|:----:|-----------|-------------|
| 1ï¸âƒ£ | ğŸ”— URL Removal | Remove `http`, `www`, `https` links |
| 2ï¸âƒ£ | ğŸ“› Mention/Hashtag Removal | Remove `@` and `#` tags |
| 3ï¸âƒ£ | ğŸ”¤ English Removal | Remove English characters & digits |
| 4ï¸âƒ£ | â– Underscore Removal | Remove underscore characters |
| 5ï¸âƒ£ | ğŸ” Repeated Char Reduction | Normalize repeated characters |
| 6ï¸âƒ£ | ğŸ”„ Arabic Normalization | `Ø¥Ø£Ø¢Ø§` â†’ `Ø§`, `Ø©` â†’ `Ù‡`, `Ù‰` â†’ `ÙŠ` |
| 7ï¸âƒ£ | ğŸ§¹ Non-Arabic Removal | Keep only Arabic text & whitespace |
| 8ï¸âƒ£ | ğŸ“ Whitespace Normalization | Trim & normalize spaces |

---

## ğŸ—ï¸ Model Architectures

### 1ï¸âƒ£ BiLSTM with CAMeL-BERT Embeddings

```
ğŸ“¥ Input Text
    â†“
ğŸ”¤ CAMeL-BERT Tokenizer (max_length=60)
    â†“
ğŸ§  CAMeL-BERT Encoder â†’ Word Embeddings (768-dim)
    â†“
ğŸ” Bidirectional LSTM (hidden_dim=128, 1 layer)
    â†“
ğŸ’§ Dropout (0.5)
    â†“
ğŸ”¢ Linear Layer (256 â†’ 3 classes)
    â†“
ğŸ“Š Output: Negative / Positive / Neutral
```

| âš™ï¸ Component | ğŸ“ Details |
|:-------------:|:----------:|
| Embedding | CAMeL-BERT (`last_hidden_state`, 768-dim) |
| LSTM | Bidirectional, hidden_dim=128, 1 layer |
| Dropout | 0.5 |
| Classifier | Linear (256 â†’ 3 classes) |
| Optimizer | AdamW (lr=1e-3, weight_decay=0.01) |
| Scheduler | ReduceLROnPlateau (patience=3, factor=0.5) |
| Loss | CrossEntropyLoss (label_smoothing=0.1) |
| Early Stopping | patience=5, based on validation loss |

---

### 2ï¸âƒ£ Fine-tuned CAMeL-BERT (LLM)

```
ğŸ“¥ Input Text
    â†“
ğŸ”¤ CAMeL-BERT Tokenizer (max_length=60)
    â†“
ğŸ§  CAMeL-BERT for Sequence Classification (Full Fine-tuning)
    â†“
ğŸ” Grid Search: lr âˆˆ {2e-5, 3e-5} Ã— epochs âˆˆ {3, 4}
    â†“
ğŸ“Š Output: Negative / Positive / Neutral
```

| âš™ï¸ Component | ğŸ“ Details |
|:-------------:|:----------:|
| Base Model | `CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment` |
| Task | Sequence Classification (3 classes) |
| Hyperparameter Search | lr âˆˆ {2e-5, 3e-5}, epochs âˆˆ {3, 4} |
| Batch Size | 16 |
| Weight Decay | 0.01 |
| Label Smoothing | 0.1 |
| Best Model Selection | Based on validation F1 (macro) |

---

## ğŸ“ˆ Results

### ğŸ† Test Set Performance (HIA Qatar Airport Tweets)

<div align="center">

| ğŸ¤– Model | ğŸ¯ Accuracy | ğŸ“Š F1 Score (Macro) |
|:---------:|:----------:|:------------------:|
| **BiLSTM + CAMeL-BERT** | **0.7616** âœ… | **0.7464** âœ… |
| **Fine-tuned LLM** | 0.7550 | 0.7407 |

</div>

### ğŸ“‹ BiLSTM â€” Detailed Classification Report

| Class | Precision | Recall | F1-Score | Support |
|:-----:|:---------:|:------:|:--------:|:-------:|
| ğŸ˜¡ Negative | 0.94 | 0.72 | 0.81 | 82 |
| ğŸ˜Š Positive | 0.82 | 0.80 | 0.81 | 40 |
| ğŸ˜ Neutral | 0.49 | 0.83 | 0.62 | 29 |

### ğŸ“‹ Fine-tuned LLM â€” Detailed Classification Report

| Class | Precision | Recall | F1-Score | Support |
|:-----:|:---------:|:------:|:--------:|:-------:|
| ğŸ˜¡ Negative | 0.97 | 0.68 | 0.80 | 82 |
| ğŸ˜Š Positive | 0.68 | 0.85 | 0.76 | 40 |
| ğŸ˜ Neutral | 0.56 | 0.83 | 0.67 | 29 |

---

## âš™ï¸ Requirements

```txt
torch
transformers
pandas
numpy
scikit-learn
matplotlib
seaborn
openpyxl
```

---

## ğŸš€ How to Run

### 1. Clone the repository
```bash
git clone <repository-url>
cd Arabic-Sentiment-Analysis
```

### 2. Install dependencies
```bash
pip install torch transformers pandas numpy scikit-learn matplotlib seaborn openpyxl
```

### 3. Ensure GPU availability âš¡
```python
import torch
print(torch.cuda.is_available())  # Should be True âœ…
```

### 4. Run the notebook ğŸ““
- Open `Arabic-Sentiment-Analysis.ipynb` in **Jupyter Notebook** or **Kaggle**
- Execute all cells sequentially

> ğŸ’¡ **Tip:** This project was originally developed and run on **Kaggle** with GPU acceleration. Dataset paths may need to be adjusted if running locally.

---

## âœ¨ Key Features

| Feature | Description |
|:-------:|:-----------:|
| ğŸ”’ **Reproducibility** | Random seed fixed at `42` for deterministic results |
| ğŸ‡¸ğŸ‡¦ **Arabic-specific** | Comprehensive text normalization pipeline for Arabic |
| ğŸ”„ **Cross-domain** | Training on AraSenti, testing on airport tweets |
| ğŸ” **Hyperparameter Search** | Grid search over learning rates and epochs |
| âš–ï¸ **Model Comparison** | Side-by-side evaluation with confusion matrices |
| ğŸ“Š **Visualization** | Training curves & confusion matrix heatmaps |

---

## ğŸ› ï¸ Technologies Used

<div align="center">

| Technology | Purpose |
|:----------:|:-------:|
| ğŸ **Python 3** | Programming Language |
| ğŸ”¥ **PyTorch** | Deep Learning Framework |
| ğŸ¤— **Hugging Face Transformers** | Pre-trained Models & Fine-tuning |
| ğŸª **CAMeL-BERT** | Arabic Language Model |
| ğŸ“Š **scikit-learn** | Evaluation Metrics |
| ğŸ“ˆ **Matplotlib & Seaborn** | Data Visualization |

</div>

---

<div align="center">

### ğŸ“œ License

This project is developed for **academic purposes** as part of the **Natural Language Processing** course.

---

â­ **If you found this project helpful, please give it a star!** â­

</div>
]]>
